---
title: "Downloading, cleaning and mapping data from GBIF"
authors: 
  -Florencia Grattarola
  -Friederike Woelke
  -Gabriel Ortega

format: docx
editor: visual
---

# Mammal's of the Czech Republic

We will use the mammals of Czech Republic as an example dataset. We will access data through [**GBIF**](https://gbif.org) using tools available in R.

## Some preparation before starting to code

-   Create a new project for all your practical sessions (with a Scripts and Data folder inside).
-   Comment your code as much as possible, as if you were to explain it to others (that other could be you in 3 months!).
-   Keep your code short and easily readable in plain English.

# Install and load libraries

We will use the long way to install and load packages. I'll recommend you to investigate the short way using the function `p_load()` from the package **`pacman`**

## Install the package `tidyverse`

Tidyverse is a set of libraries for easier manipulation and transformation of data. We will be using many functions from this package, like `filter()`, `mutate()`, and later `read_csv()`.

```{r eval=FALSE}
install.packages("tidyverse") # install
```

```{r}
library(tidyverse) # load
```

## Install the package `rgbif`

We will use **`rgbif`** to download data from GBIF directly into our R session. Rgbif is plenty of functions that allow us to interact with GBIF.

```{r eval=FALSE}
install.packages("rgbif")
```

To use it, we load the library and check it's working.

```{r}
library(rgbif)
packageVersion("rgbif")
```

We will need the GBIF backbone taxon ID (`taxonKey`) for the *Mammalia* class. For that we will use another package called **`taxize`**.

```{r eval=FALSE}
install.packages("taxize")
```

Same as with **rgbif,** load **taxize** and check it's working.

```{r}
library(taxize)
packageVersion("taxize")
```

# Project variables

It is advisable to have a section of project/work variables in your scripts. That way it will be easy to reuse your scripts in other tasks.

## Create your project variables

Define here the things you already know you will use later on the script. For instance, we know that we will work with **mammals** from **Czechia** and the data we will get from GBIF are in **WGS84** latitude and longitude.

```{r}
taxa <- "Mammalia"
country_code <- "CZ" # Two letters ISO code for Czechia
proj_crs <- 4326 # EPSG code for WGS84
```

Let's get the taxon ID for the *Mammalia* class

```{r}
taxon_key <- get_gbifid_(taxa) %>%
  bind_rows() %>% # Transform the result of get_gbifid into a dataframe
  filter(matchtype == "EXACT" & status == "ACCEPTED") %>% # Filter the dataframe by the columns "matchtype" and "status"
  pull(usagekey) # Pull the contents of the column "usagekey"
```

It is often useful to have a base map of the study area (in this case the whole Czechia).

```{r warning=FALSE,message=FALSE}
base_map <- rnaturalearth::ne_countries(
  scale = 110,
  type = "countries",
  country = "czechia",
  returnclass = "sf"
)
```

# Data download

And now we can use the function `occ_count()` to find out the **number of occurrence records** for Czechia.

How many occurrence records are in GBIF for the entire **Czechia**?

```{r}
occ_count(country = country_code)
```

And how many records for the **mammals** of Czech Republic?

```{r}
occ_count(
  country = country_code,
  taxonKey = taxon_key
)
```

Now we are ready to do a download. To do this, we will use `occ_search()`. This function has many options that correspond with fields in GBIF database.

``` r
occ_search(
  taxonKey = NULL, # We will put here our taxon_key
  scientificName = NULL,
  country = NULL, # We will put here our country_code
  publishingCountry = NULL,
  hasCoordinate = NULL, 
  typeStatus = NULL, 
  recordNumber = NULL, 
  lastInterpreted = NULL, 
  continent = NULL, 
  geometry = NULL, 
  geom_big = "asis", 
  geom_size = 40, 
  geom_n = 10, 
  recordedBy = NULL, 
  recordedByID = NULL, 
  identifiedByID = NULL, 
  basisOfRecord = NULL, # Useful for filtering the type of record you want
  datasetKey = NULL, 
  eventDate = NULL, 
  catalogNumber = NULL, 
  year = NULL, 
  month = NULL, 
  decimalLatitude = NULL, 
  decimalLongitude = NULL, 
  elevation = NULL, 
  depth = NULL, 
  institutionCode = NULL, 
  collectionCode = NULL, 
  hasGeospatialIssue = NULL, # We will use this to retrieve only records without known geospatial issues
  issue = NULL, 
  search = NULL, 
  mediaType = NULL, 
  subgenusKey = NULL, 
  repatriated = NULL, 
  phylumKey = NULL, 
  kingdomKey = NULL, 
  classKey = NULL, 
  orderKey = NULL, 
  familyKey = NULL, 
  genusKey = NULL, 
  establishmentMeans = NULL, 
  protocol = NULL, 
  license = NULL, 
  organismId = NULL, 
  publishingOrg = NULL, 
  stateProvince = NULL, 
  waterBody = NULL, 
  locality = NULL, 
  limit = 500, # we will modify this to get the amount of records we need
  start = 0, 
  fields = "all", 
  return = NULL, 
  facet = NULL, 
  facetMincount = NULL, 
  facetMultiselect = NULL, 
  skip_validate = TRUE, 
  curlopts = list(), ...
)
```

------------------------------------------------------------------------

## Data download through R

Get occurrences records of mammals from Czech Republic.

```{r}
mammalsCZ <- occ_search(
  taxonKey = taxon_key, # Key 359 created previously
  country = country_code, # CZ, ISO code of Czechia
  limit = 6000, # Max number of records to download
  hasGeospatialIssue = F # Only records without spatial issues
)

mammalsCZ <- mammalsCZ$data # The output of occ_search is a list with a data object inside. Here we pull the data out of the list.
```

# Data exploration

Examine the dataset's variables and their respective data types: Are they numeric, character, or boolean in nature? That will tell you what you can do with every column in the dataset.

```{r eval=FALSE}
glimpse(mammalsCZ)
```

How many records do we have?

```{r}
nrow(mammalsCZ)
```

How many species do we have?

```{r}
mammalsCZ %>%
  filter(taxonRank == "SPECIES") %>%
  distinct(scientificName) %>%
  nrow()
```

`distinct()` is used to see unique values

# Data quality

Data are not 'good' or 'bad', the quality will depend on our goal. Some things we can check:

-   Base of the record (type of occurrence)
-   Species names (taxonomic harmonisation)
-   Spatial and temporal (accuracy / precision)

![](https://docs.ropensci.org/CoordinateCleaner/logo.png){width="45"} `CoordinateCleaner`: <https://github.com/ropensci/CoordinateCleaner>

CoordinateCleaner offers advanced data filtering and cleaning capabilities by automatically identifying and flagging common spatial and temporal errors.

## Basic data filtering

As an example of data cleaning procedures, we will check the following fields in our dataset:

-   `basisOfRecord`: we want preserved specimens or observations

-   `taxonRank`: we want records at species level.

-   `coordinateUncertaintyInMeters`: we want them to be smaller than 10km.

### `basisOfRecord`:

We want preserved specimens or observations

```{r}
mammalsCZ %>% distinct(basisOfRecord) %>% knitr::kable()
```

Count how many records are in each type of *basisOfRecord*

```{r}
mammalsCZ %>%
  group_by(basisOfRecord) %>%
  count() %>% 
  knitr::kable()
```

`group_by()` is used to group values within a variable

Filter *basisOfRecord* to get only records that correspond to "preserved specimens" or "human observations".

```{r}
mammalsCZ <- mammalsCZ %>%
  filter(basisOfRecord == "PRESERVED_SPECIMEN" |
    basisOfRecord == "HUMAN_OBSERVATION")
```

Note the use of `|` (OR) to filter the data. Another alternative is `filter(basisOfRecord %in% c("PRESERVED_SPECIMEN","HUMAN_OBSERVATION"))`

How many records do we have now?

```{r}
nrow(mammalsCZ)
```

### `taxonRank`:

We want records at species level

```{r}
mammalsCZ %>% distinct(taxonRank) %>% knitr::kable()
```

Let's get records at species level

```{r}
mammalsCZ <- mammalsCZ %>%
  filter(taxonRank == "SPECIES")
```

How many records do we have now?

```{r}
nrow(mammalsCZ)
```

### `coordinateUncertaintyInMeters`:

We want uncertainty to be smaller than 10km.

Are there records with precision larger than 10km?

```{r}
mammalsCZ %>%
  filter(coordinateUncertaintyInMeters >= 10000) %>%
  select(scientificName, 
         coordinateUncertaintyInMeters, 
         stateProvince) %>% 
  knitr::kable()
```

Let's keep records with precision smaller than 10km.

```{r}
mammalsCZ <- mammalsCZ %>%
  filter(coordinateUncertaintyInMeters < 10000) # keeping this
```

How many records do we have now?

```{r}
nrow(mammalsCZ)
```

# Basic maps

## How are the records distributed?

```{r}
library(sf)

mammalsCZ_sf <- mammalsCZ %>%
  filter(!is.na(decimalLongitude) &
    !is.na(decimalLatitude)) %>%
  st_as_sf(coords = c(
    "decimalLongitude",
    "decimalLatitude"
  )) %>%
  st_set_crs(proj_crs)

ggplot() +
  geom_sf(data = base_map, fill = "white") +
  geom_sf(
    data = sf::st_intersection(mammalsCZ_sf, base_map),
    aes(col = order)
  ) +
  theme_bw()
```

And finally, a simple trick to produce separate maps per order.

```{r}
ggplot() +
  geom_sf(data = base_map, fill = "white") +
  geom_sf(
    data = sf::st_intersection(mammalsCZ_sf, base_map),
    aes(col = order)
  ) +
  theme_bw() +
  # Added to the previous ggplot
  facet_wrap(~order) +
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)
  )
```
